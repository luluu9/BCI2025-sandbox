{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "data_path = r\"data\\first_try\\mati\\mati_test_real_movement_1_annotated.fif\"\n",
    "is_real_movement = \"real\" in data_path.lower()\n",
    "recording_type = \"real_movement\" if is_real_movement else \"motor_imagery\"\n",
    "\n",
    "raw = mne.io.read_raw_fif(data_path, preload=True)\n",
    "eeg_channels = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\"]\n",
    "raw.pick(picks=eeg_channels)\n",
    "raw.resample(sfreq=250)\n",
    "raw.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin')\n",
    "raw.notch_filter(freqs=[50.0])\n",
    "\n",
    "mateusz_mapping = {\n",
    "    'A1': 'Fp1',\n",
    "    'A2': 'Fp2',\n",
    "    'A3': 'F4',\n",
    "    'A4': 'Fz',\n",
    "    'A5': 'F3',\n",
    "    'A6': 'T7',\n",
    "    'A7': 'C3',\n",
    "    'A8': 'Cz',\n",
    "    'A9': 'C4',\n",
    "    'A10': 'T8',\n",
    "    'A11': 'P4',\n",
    "    'A12': 'Pz',\n",
    "    'A13': 'P3',\n",
    "    'A14': 'O1',\n",
    "    'A15': 'Oz',\n",
    "    'A16': 'O2'\n",
    "}\n",
    "\n",
    "hania_mapping = {\n",
    "    'A1': 'Fp1',\n",
    "    'A2': 'Fp2',\n",
    "    'A3': 'F4',\n",
    "    'A4': 'Fz',\n",
    "    'A5': 'F3',\n",
    "    'A6': 'CP1', # modified\n",
    "    'A7': 'C3', \n",
    "    'A8': 'Cz',\n",
    "    'A9': 'C4',\n",
    "    'A10': 'CP2', # modified\n",
    "    'A11': 'P4',\n",
    "    'A12': 'Pz',\n",
    "    'A13': 'P3',\n",
    "    'A14': 'FC1', # modified\n",
    "    'A15': 'CPz', # modified // not sure if this is exactly correct\n",
    "    'A16': 'FC2' # modified\n",
    "}\n",
    "\n",
    "raw.rename_channels(mateusz_mapping)\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "raw.set_montage(montage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events, all_events_id = mne.events_from_annotations(raw)\n",
    "print(all_events)\n",
    "print(all_events_id)\n",
    "\n",
    "# rename event ids to more readable names\n",
    "all_events_id = {'Both feets': 1, 'Both hands': 2, 'Left hand': 3, 'Relax': 4, 'Right hand': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch data\n",
    "There is issue with varying epoch-time - in our data relax takes 5s, and other samples 10s. MNE does not support multiple epoch times.\\\n",
    "Simple fix is to drop relax for now - we can create second epoch object with epochtime=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Relax' in all_events_id:\n",
    "    relax_event_id = {'Relax': all_events_id['Relax']}\n",
    "    del all_events_id['Relax']\n",
    "\n",
    "task_margin = 1.0 # seconds, margin to be sure that we are focused on the task\n",
    "task_end = 9.0 # seconds, to have same length for all epochs\n",
    "epochs = mne.Epochs(\n",
    "    raw=raw,\n",
    "    events=all_events,\n",
    "    event_id=all_events_id,\n",
    "    baseline=None,\n",
    "    tmin=task_margin,\n",
    "    tmax=task_end,\n",
    "    preload=True\n",
    ")\n",
    "\n",
    "task_end = 4.0\n",
    "relax_epochs = mne.Epochs(\n",
    "    raw=raw,\n",
    "    events=all_events,\n",
    "    event_id=relax_event_id,\n",
    "    baseline=None,\n",
    "    tmin=task_margin,\n",
    "    tmax=task_end,\n",
    "    preload=True\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 15))\n",
    "\n",
    "for i, event_id in enumerate(all_events_id):\n",
    "    epochs[event_id].average().plot(gfp=True, spatial_colors=True, titles={\"eeg\": event_id}, axes=axes[i], show=False)\n",
    "\n",
    "relax_epochs.average().plot(gfp=True, spatial_colors=True, titles={\"eeg\": \"Relax\"}, axes=axes[4], show=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot topography of each band for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(15, 10))\n",
    "\n",
    "for i, mode in enumerate(all_events_id):\n",
    "    epochs[mode].compute_psd().plot_topomap(axes=axes[i], show=False)\n",
    "    axes[i, 0].set_ylabel(mode, rotation=90, fontsize=12, fontweight='bold')\n",
    "\n",
    "relax_epochs.compute_psd().plot_topomap(axes=axes[4], show=False)\n",
    "axes[4, 0].set_ylabel('Relax', rotation=90, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate second epoch\n",
    "For that, ffmpeg should be installed: https://www.ffmpeg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_time = task_end - task_margin\n",
    "step = 0.004\n",
    "slowdown = 20 # each frame will be displayed 20 times longer, 8 sec recording to 160 second animation\n",
    "times = [v*step for v in range(int(task_margin/step), int(task_end/step))]\n",
    "framerate = 1/(step*slowdown)\n",
    "filenames = []\n",
    "for mode in [\"Left hand\", \"Right hand\"]:\n",
    "    recording = epochs[mode][1].average()\n",
    "    fig, anim = recording.animate_topomap(times=times, ch_type=\"eeg\", frame_rate=framerate, blit=False, show=False)\n",
    "    filename = f\"visualisation_out/{recording_type}_{mode}.mp4\"\n",
    "    anim.save(filename=filename, writer=\"ffmpeg\")\n",
    "    filenames.append(filename)\n",
    "    print(f\"end of saving [{mode}] animation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine two videos together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def combine_videos(left_video_path, right_video_path, output_path):\n",
    "    cmd = f'ffmpeg -i \\\"{left_video_path}\\\" -i \\\"{right_video_path}\\\" -filter_complex hstack \\\"{output_path}\\\"'\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "left_video = filenames[0]\n",
    "right_video = filenames[1]\n",
    "output_video = \"visualisation_out/left_and_right_hand.mp4\"\n",
    "\n",
    "combine_videos(left_video, right_video, output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show epoch's most active regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_left = len(epochs['Left hand'])\n",
    "n_right = len(epochs['Right hand'])\n",
    "n_epochs = hania_mapping(n_left, n_right)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(5, 2), width_ratios=[12, 12, 1])\n",
    "    for ax, mode in zip(axes, ['Left hand', 'Right hand']):\n",
    "        evoked = epochs[mode][i].average()\n",
    "        evoked.plot_topomap(times=5, ch_type='eeg', axes=[ax, axes[2]], show=False, average=8)\n",
    "        ax.set_title(f\"{mode} - epoch {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show record's most active regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(7, 2), width_ratios=[12, 12, 1])\n",
    "recording = epochs[mode].average()\n",
    "for ax, mode in zip(axes, ['Left hand', 'Right hand']):\n",
    "    evoked = epochs[mode].average()\n",
    "    evoked.plot_topomap(times=5, ch_type='eeg', axes=[ax, axes[2]], show=False, average=8)\n",
    "    ax.set_title(f\"{mode} - epochs average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show step-by-step comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.05\n",
    "times = [v * step for v in range(int(task_margin / step), int(2.0 / step))]  # up to 2 second, as blink happened after that in this recording and it distorts the scale\n",
    "n_cols = len(times) + 1  # extra column for colorbar\n",
    "fig2, axes2 = plt.subplots(2, n_cols, figsize=(len(times) * 2, 4), width_ratios=[5] * len(times) + [1])\n",
    "for i, mode_name in enumerate(['Left hand', 'Right hand']):\n",
    "    evoked = epochs[mode_name].average()\n",
    "    evoked.plot_topomap(times=times, ch_type=\"eeg\", axes=axes2[i, :], show=False, average=step)\n",
    "    axes2[i, 0].set_ylabel(mode_name, rotation=90, fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
